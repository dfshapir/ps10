---
title: "Problem Set 10"
author: "Daniel Shapiro"
date: "11/17/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 0)
set.seed(6800)
library(tidyverse)
library(stats)
library(foreign)
library(ggfortify)
library(sandwich)
library(estimatr)
```

### Question 1 Background:

*We'll use some data on crime to evaluate and correct for issues with our standard errors, using the \texttt{prison.Rdata} dataset available on the course website. Note that there is not one right way to model this question - your answers can legitimately be different from one anothers', though the steps taken and the types of arguments made should be similar.*

```{r}
load('prison.Rdata')
head(prison)
```

### 1a) Test the hypothesis that the violent crime rate is linked to unemployment (\texttt{unem}), controlling for per capita income (\texttt{incpc}), prison population (\texttt{pris}), police presence (\texttt{polpc}), percent living in an urban area (\texttt{metro}), and the 4 age categories. Briefly interpret the coefficients from this regression, and describe whether the model meets the OLS assumptions. Evaluate the residuals using the plots produced by \texttt{autoplot}.

```{r}
model <- lm(criv ~ unem + incpc + pris + polpc + metro + `ag0_14` + `ag15_17` + `ag18_24` + `ag25_34`, data = prison)
summary(model)
```

It looks from this regression that unemployment has a really strong relationship with violent crime -- the coefficient is 9.4 or so. But that's a bit misleading; they're saying that for each "1" increase in unemployment, violent crime will go up by a bit over 9. But a "1" increase means a 100% increase in unemployment, because of the way that the data is structured. So really the coefficient is much lower. Nonetheless though, we can see that higher unemployment does tend to mean more violent crime. Honestly, many of these coefficients are a bit irritating to interpret; the coefficient for "incpc" measures what an increase in a single dollar does to violent crime. This stat is too minute; it would be better to look at it by thousands or by tens of thousands or something. 

The main thing we should look at is the signs and the significance. There's a negative relationship between income and violent crime, and pretty strong positive correlations between prisons, police presence, and living in a city. There are mixed results for age.

The model doesn't really fit a lot of the OLS assumptions. One major one that we can see right off the bat without running data analysis is the endogeneity/exogeneity question. There are a lot of variables in here that are potentially endogenous. Even unemployment and violent crime rate could be the other way around -- it could be that a high violent crime rate reduces the likelihood that businesses will move to the area, lowering the number of jobs available and thus increasing unemployment. High police presence could be *because* of high crime rates, not the other way around. A high prison population could be *because* of high violent crime rates, not the other way around. There are a lot of variables that should really be explored further. 

Let's look at some of the autoplots.

```{r}
autoplot(model)
```

Looking at the first graph, it looks like the linearity assumption largely holds, but I'm a bit worried about the skew toward the right side of the graph. I can't tell about the second one (random sampling) -- I don't know the methodology of the paper. Variation in X seems fine; there are a ton of different values. 

Homoskedasticity -- The data definitely does **not** look homoskedastic. The scale-location plot shows this. The line does go horizontal eventually, but only for a few points, and there is a non-horizontal line for the most part. 

Normality of errors -- there are a few outliers, but in the grand scheme of things, it could be worse. I'm looking at Normal Q-Q and the residuals vs leverage graphs. The one thing that worries me is the jog upwards in the Q-Q graph towards the end. But honestly all things considered, it could be worse, in my opinion. 

### 1b) Evaluate your data to see whether any of your variables should be log transformed. Re-run the model with any transformations that you see as useful/necessary. Reevaluate the model for normality of errors after making these changes. Briefly interpret the coefficients.

I looked at histograms of all the variables to see which ones were heavily skewed one way or another with a ton of outliers. In my opinion, there were three variables that should probably be log transformed: criv, pris, and polpc. All have fairly significant outliers.

Below, I show what log transforming these does to the data.

```{r}
prison_log <- prison %>%
  mutate(logcriv = log(criv)) %>%
  mutate(logpris = log(pris)) %>%
  mutate(logpolpc = log(polpc)) %>%
  select(-c(criv, pris, polpc))

log_model <- lm(logcriv ~ unem + incpc + logpris + logpolpc + metro + `ag0_14` + `ag15_17` + `ag18_24` + `ag25_34`, data = prison_log)
summary(log_model)
```

Looking at the coefficients, it looks like not too much changed. The ones that flipped signs were already non-significant in the intial model, so I'm not entirely worried about that. The model in general ended up with many more "three star" significant variables than the initial one. The coefficients are all different than in the initial one, but that's because I log transformed the dependent variable as well, so the exact numbers will naturally changed. I'm mostly concerned about a) significance and b) if too many of the signs completely shifted.

Let's run the error tests.

```{r}
autoplot(log_model, c(2, 5))
```

Honestly, this looks a fair amount better in my opinion. It's still not perfect, but it definitely looks better than the initial model did.

### 1c) Implement White's heteroskedasticity consistent errors, and compare your results to those obtained through your chosen model in part b). Do any of our conclusions change? If so, how? Are heteroskedasticity-robust errors  appropriate here?

```{r}
hclog_model <- lm_robust(logcriv ~ unem + incpc + logpris + logpolpc + metro + `ag0_14` + `ag15_17` + `ag18_24` + `ag25_34`, data = prison_log)
summary(hclog_model)
```

The error terms definitely change; there are six that are lower than in the initial model and four that are higher. As we noted in lecture, however, Whiteâ€™s estimator does not change our estimates $\hat{\beta}$. Here, the lm_robust() function uses HC2 errors, which according to the slides, should produce pretty similar results as the others.

My sense is that heteroskedasticity-robust errors are appropriate here. Jane's slides said: "There is very little reason *not* to use robust standard errors - so as a general practice, you should always use heteroskedasticity robust SEs." I'm taking this to heart. Plus, we objectively can see that there is at least some heteroskedasticity in the data due to the last autoplot() tests in the previous question, even though we minimized it at least a bit.

### 1d) Note that these data include multiple observations from each state. What are we assuming when not accounting for this fact? Adjust your regression results for this issue. What, if anything, has changed?

This is an interesting point. When we don't account for this fact, we are essentially assuming that the state doesn't really matter for the purposes of this experiment. So if I control for state, let's see what happens.

```{r}
hclog_model_state <- lm_robust(logcriv ~ unem + incpc + logpris + logpolpc + metro + `ag0_14` + `ag15_17` + `ag18_24` + `ag25_34` + state, data = prison_log)
summary(hclog_model_state)
```

This really doesn't do all that much of anything. Also, the coefficient for state is essentially useless; the state variable is coded as numbers, so this just shows that apparently states that get coded with higher numbers on average tend to have higher crime rates than states coded with lower numbers. Since we don't know what all goes into the coding, it doesn't really tell us anything decipherable at the moment.

### 1e) Find your clustered standard errors using a block bootstrap with 1,000 simulations. Briefly discuss whether, how, and why your standard errors change from part d).

I'm going to base this off of the example we did in class.

```{r}
set.seed(6800)

coefv <- c()

for (i in 1:1000){
  
  s_cluster <- sample(unique(prison_log$state),
                      length(unique(prison_log$state)), replace = T)
  bb_data <- matrix(nrow = 0, ncol = length(colnames(prison_log)))
  colnames(bb_data) <- colnames(prison_log)
  
for (cluster in s_cluster){
  new_data <- subset(prison_log, state == cluster)
  bb_data <- rbind(bb_data, new_data)
}
  mod <- lm(logcriv ~ unem + incpc + logpris + logpolpc + metro + `ag0_14` + `ag15_17` + `ag18_24` + `ag25_34` + state, data = bb_data)
  coefv <- c(coefv, mod$coefficients[2])
}
```

```{r}
sd(coefv)
```

This is the SE value for unemployment (unem). It's a bit bigger than the corresponding value for 1d, meaning that the estimate is a bit more conservative. There's a pretty small number of clusters and a relatively high level of within-cluster correlation, so that could explain this outcome.

### Question 2 Background:

*We will revisit the \texttt{x.csv} data we used previously. In this problem, we are going to explore the case when the error term is not normally distributed and the case of heteroskedasticity. We keep the same population regression model:*

$$
y_i=3+5x_i+u_i
$$

*except this time we generate $u$ from an exponential distribution with rate=0.5. We will also need to subtract 2 from each $u$ to make sure it has the expectation of zero. Treat $x$ as fixed, meaning your random samples should use the same $x$ with different errors.*

### 2a) Simulate the sampling distributions for $\hat\beta_0$ and $\hat\beta_1$ by doing the following 1,000 times:

\begin{enumerate}
\item Generate random errors from $u$ from an exponential distribution with rate =0.5 and subtract 2 from each $u$
\item Generate values for $y$ using $u$, the fixed $x$, and the true population parameters
\item Run a regression of $y$ on $x$
\item Record your OLS estimates for coefficients and standard errors
\item Repeat
\end{enumerate}

```{r}
data2 <- read.csv("x.csv")
```

This is very similar to question 1 in Problem Set 8, but the "u" is created in a different way. I use similar code from that with the new parameters for u.

```{r}
# Create empty dataframe 

dataframe <- data.frame(matrix(ncol = 4, nrow = 1000))

for(i in 1:1000){
maindata <- data2 %>%
  mutate(u = rexp(1000, rate = 0.5) - 2) %>%
  mutate(y = (3 + 5*`x` + `u`))

regression <- lm(y ~ x, data = maindata)

m <- tidy(regression)

dataframe$X1[i] <- m[1,2][[1]]

dataframe$X2[i] <- m[2,2][[1]]

dataframe$X3[i] <- m[1,3][[1]]

dataframe$X4[i] <- m[2,3][[1]]
}
```

### Compare your sampling distribution and the standard errors you get from your regression. How do the two compare?

```{r}
sd(dataframe$X1); mean(dataframe$X3)
```

```{r}
sd(dataframe$X2); mean(dataframe$X4)
```

The standard deviations of the sampling distributions are fairly close to those of the mean of our regressions' standard errors. 

### 2b) Repeat a), but with $n=5$ (using only the first 5 observations for $x$). Compare your results with part a), and why you see any changes.

Again, this is pretty similar to what we did on Problem Set 8, but the errors are different.

```{r}
dataframe2 <- data.frame(matrix(ncol = 4, nrow = 1000))

for(i in 1:1000){
maindata <- data2 %>%
  mutate(u = rexp(1000, rate = 0.5) - 2) %>%
  mutate(y = (3 + 5*`x` + `u`))

newbdata <- maindata[1:5,]

regression <- lm(y ~ x, data = newbdata)

m <- tidy(regression)

dataframe2$X1[i] <- m[1,2][[1]]

dataframe2$X2[i] <- m[2,2][[1]]

dataframe2$X3[i] <- m[1,3][[1]]

dataframe2$X4[i] <- m[2,3][[1]]
}
```

Now, we see how the errors match up.

```{r}
sd(dataframe2$X1); mean(dataframe2$X3)
```

```{r}
sd(dataframe2$X2); mean(dataframe2$X4)
```

These errors are much farther away from one another. This makes sense, given that our sample size is so much smaller. 

### 2c) Now we create a set of values $u$ such that your errors will be heteroskedastic - the dispersion of the error term is a function of $x$ while the mean is zero (so as to satisfy the zero mean assumption):

```{r}
x <- read.csv('x.csv')$x
u <- runif(length(x), -abs(x), abs(x))

cdata <- cbind(x, u)
cdata <- as.data.frame(cdata)
```

### Plot your errors against the $x$ values to demonstrate that your errors are indeed heteroskedastic. Repeat part (a) and (b) using this new $u$. Describe your results. Why are you getting results like this?

```{r}
ggplot(cdata, aes(x = x, y = u)) +
  geom_point() +
  geom_smooth(method = "loess")
```

The errors are indeed heteroskedastic.

Now, we're again going to do similar stuff as in Problem Set 8.

```{r}
dataframe3 <- data.frame(matrix(ncol = 4, nrow = 1000))

for(i in 1:1000){
maindata <- data2 %>%
  mutate(u = runif(length(x), -abs(x), abs(x))) %>%
  mutate(y = (3 + 5*`x` + `u`))

regression <- lm(y ~ x, data = maindata)

m <- tidy(regression)

dataframe3$X1[i] <- m[1,2][[1]]

dataframe3$X2[i] <- m[2,2][[1]]

dataframe3$X3[i] <- m[1,3][[1]]

dataframe3$X4[i] <- m[2,3][[1]]
}
```

```{r}
sd(dataframe3$X1); mean(dataframe3$X3)
```

```{r}
sd(dataframe3$X2); mean(dataframe3$X4)
```

```{r}
dataframe4 <- data.frame(matrix(ncol = 4, nrow = 1000))

for(i in 1:1000){
maindata <- data2 %>%
  mutate(u = runif(length(x), -abs(x), abs(x))) %>%
  mutate(y = (3 + 5*`x` + `u`))

newcdata <- maindata[1:5,]

regression <- lm(y ~ x, data = newcdata)

m <- tidy(regression)

dataframe4$X1[i] <- m[1,2][[1]]

dataframe4$X2[i] <- m[2,2][[1]]

dataframe4$X3[i] <- m[1,3][[1]]

dataframe4$X4[i] <- m[2,3][[1]]
}
```

```{r}
sd(dataframe4$X1); mean(dataframe4$X3)
```

```{r}
sd(dataframe4$X2); mean(dataframe4$X4)
```

The errors overall get smaller compared to their counterparts in part a) and b), because we've changed our parameters for how the error terms are constructed. But what's mainly interesting is that heteroskedastic errors affect $\hat{\beta_{1}}$ much more than for $\hat{\beta_{0}}$. 

### 2d) Repeat part c) using heteroskedasticity-robust standard errors for your regression of $y$ on $x$. Compare the standard errors from your regressions to those derived from the bootstrapped sampling distribution. How do your results differ from part c)? Why?

```{r}
dataframe5 <- data.frame(matrix(ncol = 4, nrow = 1000))

for(i in 1:1000){
maindata <- data2 %>%
  mutate(u = runif(length(x), -abs(x), abs(x))) %>%
  mutate(y = (3 + 5*`x` + `u`))

regression <- lm_robust(y ~ x, data = maindata)

m <- tidy(regression)

dataframe5$X1[i] <- m[1,2][[1]]

dataframe5$X2[i] <- m[2,2][[1]]

dataframe5$X3[i] <- m[1,3][[1]]

dataframe5$X4[i] <- m[2,3][[1]]
}
```

```{r}
sd(dataframe5$X1); mean(dataframe5$X3)
```

```{r}
sd(dataframe5$X2); mean(dataframe5$X4)
```

```{r}
dataframe6 <- data.frame(matrix(ncol = 4, nrow = 1000))

for(i in 1:1000){
maindata <- data2 %>%
  mutate(u = runif(length(x), -abs(x), abs(x))) %>%
  mutate(y = (3 + 5*`x` + `u`))

newddata <- maindata[1:5,]

regression <- lm_robust(y ~ x, data = newddata)

m <- tidy(regression)

dataframe6$X1[i] <- m[1,2][[1]]

dataframe6$X2[i] <- m[2,2][[1]]

dataframe6$X3[i] <- m[1,3][[1]]

dataframe6$X4[i] <- m[2,3][[1]]
}
```

```{r}
sd(dataframe6$X1); mean(dataframe6$X3)
```

```{r}
sd(dataframe6$X2); mean(dataframe6$X4)
```

The main difference here is that the errors for the first model (with the high n) for $\hat{\beta_{1}}$ end up evening out. This is because we use the robust modeling. It doesn't work as well for the second model, because we use a much smaller value of n.